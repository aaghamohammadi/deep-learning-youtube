{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In supervised learning, one common approach is classification. This involves a model making decisions based on discrete values.\n",
        "\n",
        "In this particular session, our focus will be on **binary classification**, a subtype of classification problems.\n",
        "\n",
        "Binary classification deals with situations where the outcome can only be one of two possible options. Here are some examples:\n",
        "\n",
        "- Identifying whether an image features a dog or a cat.\n",
        "- Using a medical record to determine if a person's tumor is benign or malignant.\n",
        "- Categorizing an email as either spam or not spam."
      ],
      "metadata": {
        "id": "Aq106fPEnn4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our study of deep neural networks, we've learned that a network with $K$ layers has the following structure:\n",
        "\n",
        "\\begin{align*}\n",
        "h_1 & = \\text{ReLU}(\\beta_0 + \\mathbf{\\Omega}_0x), \\\\\n",
        "h_2 & = \\text{ReLU}(\\beta_1 + \\mathbf{\\Omega}_1h_1), \\\\\n",
        "h_3 & = \\text{ReLU}(\\beta_2 + \\mathbf{\\Omega}_2h_2), \\\\\n",
        "& \\vdots \\\\\n",
        "h_K & = \\text{ReLU}(\\beta_{K-1} + \\mathbf{\\Omega}_{K-1}h_{K-1}), \\\\\n",
        "y & = \\beta_K + \\mathbf{\\Omega}_K h_K.\n",
        "\\end{align*}\n",
        "\n",
        "However, the output $y$ is a real number, which is not suitable for binary classification tasks.\n",
        "\n",
        "So, how can we transform the value of $y$ to make it suitable for binary classification? This is where the **sigmoid** activation function comes into play. The sigmoid function, defined as\n",
        "\n",
        "$$h(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "can transform any real-valued number into a value between 0 and 1, making it ideal for binary classification tasks."
      ],
      "metadata": {
        "id": "KNy71sPzqWWU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_5BA986kXqs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def plot_sigmoid():\n",
        "    \"\"\"Plot the sigmoid activation function.\"\"\"\n",
        "    z = np.linspace(-10, 10, 100)\n",
        "    h = sigmoid(z)\n",
        "\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    plt.plot(z, h)\n",
        "    plt.scatter([0], [0.5], color='red')  # Highlight the point (0, 0.5)\n",
        "    plt.text(0.2, 0.5, 'h(0)=0.5', fontsize=12, verticalalignment='bottom')  # Annotate the point (0, 0.5)\n",
        "    plt.title('Sigmoid Activation Function')\n",
        "    plt.xlabel('z')\n",
        "    plt.ylabel('h(z)')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the sigmoid activation function\n",
        "plot_sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how we can use deep neural networks in action to distinguish between a large circle containing a smaller circle in 2D."
      ],
      "metadata": {
        "id": "YeTYOqUwsyZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_data():\n",
        "    \"\"\"Generate a dataset of circles.\"\"\"\n",
        "    X, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
        "    return X, y\n",
        "\n",
        "def split_data(X, y):\n",
        "    \"\"\"Split the dataset into training and testing sets.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def plot_data(X_train, y_train):\n",
        "    \"\"\"Plot the training data.\"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
        "    plt.ylabel(\"Feature #1\")\n",
        "    plt.xlabel(\"Feature #0\")\n",
        "    plt.title(\"Training data\")\n",
        "    plt.show()\n",
        "\n",
        "# Generate and split the data\n",
        "X, y = generate_data()\n",
        "X_train, X_test, y_train, y_test = split_data(X, y)\n",
        "\n",
        "# Plot the training data\n",
        "plot_data(X_train, y_train)"
      ],
      "metadata": {
        "id": "d0_r5GRztZM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we use **test data** in addition to **training data**?\n",
        "\n",
        "Consider this analogy: imagine you're enrolled in a college course.\n",
        "\n",
        "The **training data** is akin to the exercises and sample questions you tackle throughout the semester to learn and understand the course material.\n",
        "\n",
        "On the other hand, the **test data** is like the final exam that assesses your comprehension of the course content.\n",
        "\n",
        "Just as the final exam is unseen during your study period to ensure an unbiased evaluation of your understanding, the model also shouldn't have access to the test data during its training phase.\n",
        "\n",
        "This ensures an impartial assessment of the model's performance."
      ],
      "metadata": {
        "id": "Xz7jqNH4uVd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(2,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "id": "HlPJaeHuwWqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "2sZQw0Y3xIEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_grid(X):\n",
        "    \"\"\"Create a grid of points.\"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "    return xx, yy\n",
        "\n",
        "def predict_grid(model, xx, yy):\n",
        "    \"\"\"Use the model to predict the grid points.\"\"\"\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    return Z\n",
        "\n",
        "def plot_predictions(xx, yy, Z, X_test, y_test):\n",
        "    \"\"\"Plot the contour of predictions and the test data.\"\"\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.contour(xx, yy, Z, levels=[0.5], colors='k')  # Add decision boundary\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k')\n",
        "    plt.ylabel(\"Feature #1\")\n",
        "    plt.xlabel(\"Feature #0\")\n",
        "    plt.title(\"Predictions for Test Data\")\n",
        "    plt.show()\n",
        "\n",
        "# Create a grid of points\n",
        "xx, yy = create_grid(X)\n",
        "\n",
        "# Use the model to predict the grid points\n",
        "Z = predict_grid(model, xx, yy)\n",
        "\n",
        "# Plot the contour of predictions and the test data\n",
        "plot_predictions(xx, yy, Z, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "5djMhqtCxzAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}