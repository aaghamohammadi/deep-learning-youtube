{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "When training a model, the goal is to find parameters, denoted as $\\boldsymbol{\\phi}$, that minimize a given loss function:\n",
        "\n",
        "$$\\hat{\\phi} = \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}} \\left[ L\\left[\\boldsymbol{\\phi}\\right] \\right]\n",
        "$$\n",
        "\n",
        "There are many types of optimization algorithms available. However, the standard methods for training neural networks are iterative. These algorithms start with parameters initialized using heuristic methods and then continuously adjust them to reduce the loss.\n",
        "\n",
        "One of the simplest methods in this category is **gradient descent**. This method begins with initial parameters $\\boldsymbol{\\phi} = [\\phi_0, \\phi_1, \\ldots , \\phi_N]^T$ and follows two steps:\n",
        "\n",
        "**Step 1.** Compute the derivatives of the loss with respect to the parameters:\n",
        "$$\\frac{\\partial L}{\\partial \\boldsymbol{\\phi}} = \\begin{bmatrix}\n",
        "\\frac{\\partial L}{\\partial \\phi_0} \\\\\n",
        "\\frac{\\partial L}{\\partial \\phi_1} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial L}{\\partial \\phi_N}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Step 2.** Update the parameters using the rule:\n",
        "$$\\boldsymbol{\\phi} \\leftarrow \\boldsymbol{\\phi} - \\alpha \\cdot \\frac{\\partial L}{\\partial \\boldsymbol{\\phi}}$$\n",
        "\n",
        "Here, the positive scalar $\\alpha$, known as the **learning rate**, determines the size of the updates."
      ],
      "metadata": {
        "id": "Jp7Eo8_qpqcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpPbrKdpCG2Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90], dtype=tf.float64)\n",
        "y = tf.constant([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ], dtype=tf.float64)\n",
        "\n",
        "def compute_loss(x, y, phi0, phi1):\n",
        "    # Compute the predicted values\n",
        "    y_pred = phi0 + phi1 * x\n",
        "    # Compute the squared differences\n",
        "    squared_diffs = tf.square(y_pred - y)\n",
        "    # Return the sum of the squared differences\n",
        "    return tf.reduce_sum(squared_diffs)\n",
        "\n",
        "compute_loss(x, y, 0.0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the single-variable scenario, we can formulate the loss function, often referred to as the least-squares loss, as follows:\n",
        "\n",
        "$$ \\begin{align*}\n",
        "L\\left[\\boldsymbol{\\phi}\\right] &= \\sum_{i=1}^{m} (h\\left[x_i, \\boldsymbol{\\phi}\\right] - y_i)^2 \\\\\n",
        "&= \\sum_{i=1}^{m} (\\phi_0 + \\phi_1 x_i - y_i)^2\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The derivative of the loss function with respect to the parameters can be broken down into the sum of the derivatives of each individual contribution:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\boldsymbol{\\phi}} = \\frac{\\partial}{\\partial \\boldsymbol{\\phi}} \\sum_{i=1}^{m} l_i =  \\sum_{i=1}^{m} \\frac{\\partial l_i}{\\partial \\boldsymbol{\\phi}}\n",
        "$$\n",
        "\n",
        "These individual contributions are given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial l_i}{\\partial \\boldsymbol{\\phi}} =\n",
        "\\begin{bmatrix}\n",
        "-2(y_i - (\\phi_0 + \\phi_1x_i)) \\\\\n",
        "-2x_i(y_i - (\\phi_0 + \\phi_1x_i))\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "RaUxY7XttAji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y, phi0, phi1):\n",
        "    # Compute the predicted values\n",
        "    y_pred = phi0 + phi1 * x\n",
        "    # Compute the derivatives of the loss with respect to phi0 and phi1\n",
        "    d_phi0 = -2 * tf.reduce_sum(y - y_pred)\n",
        "    d_phi1 = -2 * tf.reduce_sum((y - y_pred) * x)\n",
        "    return d_phi0, d_phi1\n",
        "\n",
        "def update_parameters(phi0, phi1, d_phi0, d_phi1, learning_rate):\n",
        "    # Update phi0 and phi1\n",
        "    phi0 = phi0 - learning_rate * d_phi0\n",
        "    phi1 = phi1 - learning_rate * d_phi1\n",
        "    return phi0, phi1\n",
        "\n",
        "def gradient_descent(x, y, phi0, phi1, learning_rate, num_iterations):\n",
        "    # Store the values of phi0, phi1, and the loss at each step\n",
        "    phi0_values = [phi0]\n",
        "    phi1_values = [phi1]\n",
        "    loss_values = [compute_loss(x, y, phi0, phi1)]\n",
        "\n",
        "    # Perform gradient descent\n",
        "    for i in range(num_iterations):\n",
        "        # Step 1: Compute the gradients\n",
        "        d_phi0, d_phi1 = compute_gradients(x, y, phi0, phi1)\n",
        "\n",
        "        # Step 2: Update phi0 and phi1 using the gradients\n",
        "        phi0, phi1 = update_parameters(phi0, phi1, d_phi0, d_phi1, learning_rate)\n",
        "\n",
        "        # Step 3: Store the new values of phi0, phi1, and the loss\n",
        "        phi0_values.append(phi0)\n",
        "        phi1_values.append(phi1)\n",
        "        loss_values.append(compute_loss(x, y, phi0, phi1))\n",
        "\n",
        "    return phi0, phi1, phi0_values, phi1_values, loss_values\n",
        "\n",
        "# Define the learning rate and the number of iterations\n",
        "learning_rate = 0.003\n",
        "num_iterations = 100\n",
        "\n",
        "# Initialize phi0 and phi1\n",
        "phi0 = 0.1\n",
        "phi1 = 0.1\n",
        "\n",
        "# Perform gradient descent\n",
        "phi0, phi1, phi0_values, phi1_values, loss_values = gradient_descent(x, y, phi0, phi1, learning_rate, num_iterations)\n",
        "\n",
        "# Print the final values of phi0 and phi1\n",
        "print(f\"Final phi0: {phi0}, Final phi1: {phi1}\")"
      ],
      "metadata": {
        "id": "XwhWqEYGUWBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a grid of phi0 and phi1 values\n",
        "phi0_grid = np.linspace(0, 1, 100)\n",
        "phi1_grid = np.linspace(0, 1, 100)\n",
        "phi0_grid, phi1_grid = np.meshgrid(phi0_grid, phi1_grid)\n",
        "\n",
        "# Compute the loss for each pair of phi0 and phi1 values\n",
        "loss_grid = np.zeros_like(phi0_grid)\n",
        "for i in range(phi0_grid.shape[0]):\n",
        "    for j in range(phi0_grid.shape[1]):\n",
        "        loss_grid[i, j] = compute_loss(x, y, phi0_grid[i, j], phi1_grid[i, j])\n",
        "\n",
        "# Create a contour plot of the loss\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.contour(phi0_grid, phi1_grid, loss_grid, levels=np.logspace(0, 5, 35), cmap='jet')\n",
        "\n",
        "# Plot the path of the gradient descent points\n",
        "plt.plot(phi0_values, phi1_values, 'r', marker='o', markersize=5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Ha1xGGIWflH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}