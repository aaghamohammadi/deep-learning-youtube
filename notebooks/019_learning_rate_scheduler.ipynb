{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In the initial stages of model training, we often employ a higher learning rate to achieve better performance.\n",
        "\n",
        "As the model progresses, we gradually reduce the learning rate.\n",
        "\n",
        "Stochastic Gradient Descent (SGD) is commonly used in practice with a learning rate schedule.\n",
        "\n",
        "At the start of the training, the learning rate ($\\alpha$) is set to a high value.\n",
        "\n",
        "This rate is then reduced by a constant factor after every N epochs.\n",
        "\n",
        "The rationale behind this approach is that during the early phases of training, the algorithm should explore the parameter space extensively, leaping from one valley to another to locate a reasonable region.\n",
        "\n",
        "Once we are in the vicinity of the optimal solution in the later stages, our focus shifts to fine-tuning the parameters.\n",
        "\n",
        "Therefore, we decrease $\\alpha$ to make more subtle adjustments."
      ],
      "metadata": {
        "id": "en4n1WX-1xHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAbvXK2tpu7P"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Define the SGD optimizer with the specified learning rate and momentum\n",
        "sgd = SGD(learning_rate=0.3, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model using SGD as the optimizer\n",
        "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.01)\n",
        "\n",
        "# Add EarlyStopping to the callbacks list\n",
        "callbacks = [early_stopping, reduce_lr]\n",
        "\n",
        "# Train the model with the ReduceLROnPlateau and EarlyStopping callbacks\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ]
    }
  ]
}