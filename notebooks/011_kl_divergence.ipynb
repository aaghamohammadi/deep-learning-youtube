{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Kullback-Leibler (KL) divergence is a measure of the difference between two probability distributions. Specifically, it quantifies how much one probability distribution diverges from a second, reference distribution.\n",
        "\n",
        "Given two probability distributions, $p(x)$ and $q(x)$, over the same random variable $X$, the KL divergence is defined as:\n",
        "\n",
        "$$\n",
        "D_{KL}(p||q) = \\mathbb{E}_{X \\sim p} \\left[ \\log \\frac{p(x)}{q(x)} \\right] = \\mathbb{E}_{X \\sim p} [\\log p(x) - \\log q(x)]\n",
        "$$"
      ],
      "metadata": {
        "id": "CAAlMcH2bl1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discrete Distributions:**\n",
        "\n",
        "$$D_{KL}(p || q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}$$\n",
        "\n",
        "**Continuous Distributions:**\n",
        "\n",
        "$$D_{KL}(p||q) = \\int_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
        "\n"
      ],
      "metadata": {
        "id": "je29GZOnoVoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kkoyrwVbegA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "def KL(P, Q):\n",
        "    epsilon = 0.00001\n",
        "    P = P + epsilon\n",
        "    Q = Q + epsilon\n",
        "    divergence = np.sum(P * np.log(P / Q))\n",
        "    return divergence\n",
        "\n",
        "def generate_samples(mu, sigma):\n",
        "    return np.random.normal(mu, sigma, 1000)\n",
        "\n",
        "def calculate_distribution(s1, s2):\n",
        "    bins = np.linspace(min(s1.min(), s2.min()), max(s1.max(), s2.max()), 100)\n",
        "    p1, _ = np.histogram(s1, bins=bins, density=True)\n",
        "    p2, _ = np.histogram(s2, bins=bins, density=True)\n",
        "    p1 /= np.sum(p1)\n",
        "    p2 /= np.sum(p2)\n",
        "    return p1, p2, bins\n",
        "\n",
        "def plot_distributions(s1, s2, bins, kl_div, dist1_label, dist2_label, title):\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.suptitle(f'KL Divergence: {kl_div:.2f}')\n",
        "\n",
        "    density1 = gaussian_kde(s1)\n",
        "    density2 = gaussian_kde(s2)\n",
        "    x = np.linspace(min(s1.min(), s2.min()), max(s1.max(), s2.max()), 1000)\n",
        "\n",
        "    plt.fill_between(x, density1(x), color='red', alpha=0.2, label=dist1_label)\n",
        "    plt.fill_between(x, density2(x), color='green', alpha=0.2, label=dist2_label)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Define two sets of observations\n",
        "s1 = generate_samples(0.2, 0.1)\n",
        "s2 = generate_samples(0, 0.1)\n",
        "\n",
        "# Calculate distributions and KL divergence\n",
        "p1, p2, bins = calculate_distribution(s1, s2)\n",
        "kl_div = KL(p1, p2)\n",
        "\n",
        "# Plot distributions\n",
        "plot_distributions(s1, s2, bins, kl_div, 'Distribution 1', 'Distribution 2', 'Similar Distributions')\n",
        "\n",
        "# Define two sets of observations\n",
        "s3 = generate_samples(0.0, 0.1)\n",
        "s4 = generate_samples(0.5, 0.1)\n",
        "\n",
        "# Calculate distributions and KL divergence\n",
        "p3, p4, bins = calculate_distribution(s3, s4)\n",
        "kl_div = KL(p3, p4)\n",
        "\n",
        "# Plot distributions\n",
        "plot_distributions(s3, s4, bins, kl_div, 'Distribution 3', 'Distribution 4', 'Different Distributions')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of **cross-entropy** has a strong connection to the KL divergence.\n",
        "\n",
        "Minimizing the cross-entropy in relation to $q$ is the same as minimizing the KL divergence in relation to $q$, given that $H(p, p)$ is not influenced by $q$."
      ],
      "metadata": {
        "id": "6dmHjSf8roes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "H(p, q) &= -\\mathbb{E}_{x \\sim p}[\\log(q(x))] \\\\\n",
        "&= -\\mathbb{E}_{x \\sim p}[\\log(p(x))] + \\mathbb{E}_{x \\sim p}[\\log(p(x))] -\\mathbb{E}_{x \\sim p}[\\log(q(x))]\\\\\n",
        "&= -\\mathbb{E}_{x \\sim p}[\\log(p(x))] + \\mathbb{E}_{x \\sim p}\\left[\\log(p(x)) -  \\log(q(x))\\right] \\\\\n",
        "&= H(p, p) + D_{KL}(p||q)\n",
        "\\end{align*}\n"
      ],
      "metadata": {
        "id": "YQbIyB8qpdor"
      }
    }
  ]
}