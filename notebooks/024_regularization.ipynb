{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization** is a technique used to reduce the discrepancy between the performance of a predictive model during training and testing. This technique enhances the model's ability to generalize.\n",
        "\n",
        "The process of regularization involves the addition of specific terms to the loss function. These terms influence the selection of optimized parameters.\n",
        "\n",
        "The basic loss function is represented as:\n",
        "\n",
        "$$\\hat{\\phi} = \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}} \\left[ \\Sigma_{i=1}^{m} l_i[x_i, y_i]\\right]\n",
        "$$\n",
        "\n",
        "To guide the minimization process towards preferred solutions, an additional term is included in the loss function:\n",
        "\n",
        "$$\\hat{\\phi} = \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}} \\left[ \\Sigma_{i=1}^{m} l_i[x_i, y_i] + \\lambda \\cdot g[\\boldsymbol{\\phi}]\\right]\n",
        "$$\n",
        "\n",
        "Here, $g[\\boldsymbol{\\phi}]$ is a function that returns a scalar. This scalar takes on larger values when the parameters are less desirable.\n",
        "\n",
        "The term $\\lambda$ is a positive scalar that determines the relative influence of the regularization term.\n",
        "\n",
        "One of the most frequently used regularization terms is the **L2 norm**, which imposes a penalty on the sum of the squares of the parameter values:\n",
        "\n",
        "$$\\hat{\\phi} = \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}} \\left[ \\Sigma_{i=1}^{m} l_i[x_i, y_i] + \\lambda \\cdot \\Sigma_{j} \\boldsymbol{\\phi_j}^2\\right]\n",
        "$$\n",
        "\n",
        "In the context of neural networks, L2 regularization is typically applied to the weights rather than the biases. Hence, it is often referred to as a **weight decay** term."
      ],
      "metadata": {
        "id": "5QRLWXVxa5Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3kfDeQjfUMII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp8ZVdVKUBCa"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "assert X_train.shape == (50000, 32, 32, 3)\n",
        "assert X_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to TensorFlow Datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    image = tf.image.resize(image, (32, 32))\n",
        "    return image, label\n",
        "\n",
        "def prepare_dataset(ds, batch_size=100):\n",
        "    return ds.map(preprocess).batch(batch_size)\n",
        "\n",
        "# Preprocess the data\n",
        "train_ds = prepare_dataset(train_ds)\n",
        "test_ds = prepare_dataset(test_ds)"
      ],
      "metadata": {
        "id": "Px0jrrEaV2Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.L2(0.005)),\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.L2(0.005)),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=tf.keras.regularizers.L2(0.005)),\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.L2(0.005)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.01, nesterov=True),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor=\"val_accuracy\", factor=0.2, patience=2, min_lr=0.001\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=5, restore_best_weights=True\n",
        ")\n",
        "callbacks = [early_stopping, reduce_lr]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds, epochs=20, validation_data=test_ds, batch_size=100, callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "ndLANVsQUWMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AQGT41TdY7lQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}