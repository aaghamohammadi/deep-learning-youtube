{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Employing gradient descent to locate the global optimum of a loss function with many dimensions can be difficult.\n",
        "\n",
        "It's possible to identify a minimum, but determining whether it's the global minimum or even a satisfactory one is impossible.\n",
        "\n",
        "A significant issue is that the end point of a gradient descent algorithm is entirely dependent on its initial point.\n",
        "\n",
        "**Stochastic gradient descent (SGD)** tries to address this by introducing some randomness into the gradient at each step."
      ],
      "metadata": {
        "id": "Ca7W-KQMhU5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The method for incorporating randomness is straightforward.\n",
        "\n",
        "In each iteration, the algorithm selects a random subset from the training data and calculates the gradient solely based on these examples.\n",
        "\n",
        "This subset is commonly referred to as a **minibatch**, or simply a **batch**.\n",
        "\n",
        "Consequently, the update rule for the model parameters $\\boldsymbol{\\phi}_t$ at the t-th iteration is as follows:\n",
        "\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\phi}_{t+1} \\leftarrow \\boldsymbol{\\phi}_t - \\alpha \\cdot \\sum_{i \\in B_t} \\frac{\\partial l_i(\\boldsymbol{\\phi}_t)}{\\partial \\boldsymbol{\\phi}}\n",
        "$$\n",
        "\n",
        "Typically, the batches are selected from the dataset without replacement.\n",
        "\n",
        "A complete pass through the entire training dataset is known as an **epoch**."
      ],
      "metadata": {
        "id": "GtRkk4hwjFXJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SdKcg7ShJnP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Data\n",
        "x = tf.constant([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90])\n",
        "y = tf.constant([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ])\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
        "    ]\n",
        ")\n",
        "# Compile\n",
        "model.compile(loss=tf.keras.losses.mse,\n",
        "              optimizer=tf.keras.optimizers.SGD())\n",
        "# Train\n",
        "model.fit(x, y, epochs=30, batch_size=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Predict\n",
        "x_values = np.linspace(0, 2, 100)\n",
        "y_pred = model.predict(x_values)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, label='Actual')\n",
        "plt.plot(x_values, y_pred, color='red', label='Predicted')\n",
        "plt.xlim([0, 2])\n",
        "plt.ylim([0, 2])\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zX5SszjDlKYX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}