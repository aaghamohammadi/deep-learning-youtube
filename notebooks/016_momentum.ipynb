{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A frequently used enhancement to stochastic gradient descent involves incorporating a **momentum** component.\n",
        "\n",
        "This involves updating the parameters using a weighted combination of the gradient calculated from the current batch and the direction taken in the preceding step.\n",
        "\n",
        "$$\n",
        "m_{t+1} \\leftarrow \\beta \\cdot m_t + (1 - \\beta) \\cdot \\sum_{i \\in B_t} \\frac{\\partial l_i(\\boldsymbol{\\phi}_t)}{\\partial \\boldsymbol{\\phi}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\phi}_{t+1} \\leftarrow \\boldsymbol{\\phi}_t - \\alpha \\cdot m_{t+1}\n",
        "$$\n",
        "\n",
        "\n",
        "The momentum helps to accelerate SGD in the relevant direction and dampens oscillations.\n",
        "\n",
        "The effective learning rate escalates when all these gradients consistently align over several iterations.\n",
        "\n",
        "However, it diminishes if the direction of the gradient frequently alters, as the terms in the summation neutralize each other."
      ],
      "metadata": {
        "id": "nG4bxc9jFTY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def load_dataset(dataset_name, split):\n",
        "    ds, ds_info = tfds.load(dataset_name, split=split, shuffle_files=True, with_info=True, as_supervised=True)\n",
        "    return ds, ds_info\n",
        "\n",
        "def shuffle_dataset(ds):\n",
        "    buffer_size = tf.data.experimental.cardinality(ds).numpy()\n",
        "    return ds.shuffle(buffer_size)\n",
        "\n",
        "def split_dataset(ds, train_ratio=0.9):\n",
        "    num_examples = tf.data.experimental.cardinality(ds).numpy()\n",
        "    num_train = int(train_ratio * num_examples)\n",
        "    train_ds = ds.take(num_train)\n",
        "    test_ds = ds.skip(num_train)\n",
        "    return train_ds, test_ds\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    return image, label\n",
        "\n",
        "def prepare_dataset(ds, batch_size=32):\n",
        "    return ds.map(preprocess).batch(batch_size)\n",
        "\n",
        "# Load the Caltech-101 dataset\n",
        "train_ds, ds_info = load_dataset('caltech101', 'train')\n",
        "test_ds, _ = load_dataset('caltech101', 'test')\n",
        "\n",
        "# Concatenate the train and test datasets\n",
        "all_ds = train_ds.concatenate(test_ds)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "all_ds = shuffle_dataset(all_ds)\n",
        "\n",
        "# Split the combined dataset into train and test datasets\n",
        "train_ds, test_ds = split_dataset(all_ds)\n",
        "\n",
        "# Prepare the datasets for training\n",
        "train_ds = prepare_dataset(train_ds)\n",
        "test_ds = prepare_dataset(test_ds)"
      ],
      "metadata": {
        "id": "flLOriOwHF38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(ds_info.features['label'].num_classes, activation='softmax')  # Update the output layer to match the number of classes in the dataset\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(momentum=0.5, learning_rate=0.001), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "n8ATAf0GkIEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_ds, epochs=20, validation_data=test_ds, batch_size=32)"
      ],
      "metadata": {
        "id": "IKzShaposBXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "SJaRnIQ9-Hxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_random_batches(test_ds, num_batches=4):\n",
        "    return np.random.choice(len(test_ds), size=num_batches, replace=False)\n",
        "\n",
        "def get_random_image_and_labels(test_ds, batch):\n",
        "    images, labels = list(test_ds)[batch]\n",
        "    index = np.random.choice(len(images), size=1)[0]\n",
        "    return images[index], labels[index]\n",
        "\n",
        "def predict_labels(model, images):\n",
        "    y_probs_batch = model.predict(images)\n",
        "    return np.argmax(y_probs_batch, axis=-1)\n",
        "\n",
        "def plot_image(ax, image, true_label, pred_label):\n",
        "    ax.set_axis_off()\n",
        "    image_to_show = (image + 1) / 2\n",
        "    ax.imshow(image_to_show.numpy(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    title = f'True: {true_label}, Pred: {pred_label}'\n",
        "    ax.set_title(title, color='green' if true_label == pred_label else 'red')\n",
        "\n",
        "def plot_images(model, test_ds, class_names):\n",
        "    batches = get_random_batches(test_ds)\n",
        "    _, axes = plt.subplots(nrows=1, ncols=len(batches), figsize=(20, 6))\n",
        "    for i, batch in enumerate(batches):\n",
        "        image, true_label = get_random_image_and_labels(test_ds, batch)\n",
        "        y_pred_batch = predict_labels(model, image[None, ...])\n",
        "        pred_label = class_names[y_pred_batch[0]]\n",
        "        true_label = class_names[true_label.numpy()]\n",
        "        plot_image(axes[i], image, true_label, pred_label)\n",
        "    plt.setp(axes, xticks=[], xticklabels=[], yticklabels=[])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get the class names from ds_info\n",
        "class_names = ds_info.features['label'].names\n",
        "\n",
        "# Plot the images with the actual and predicted labels\n",
        "plot_images(model, test_ds, class_names)"
      ],
      "metadata": {
        "id": "EJuEE9ltnhcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}