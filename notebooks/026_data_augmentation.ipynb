{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data augmentation** involves expanding the dataset to enhance performance.\n",
        "\n",
        "This technique allows us to modify each input data example in a way that preserves its original label.\n",
        "\n",
        "For instance, in an image classification task where we need to identify if a \"cat\" is present, we can apply transformations such as rotation, flipping, blurring, or adjusting the color balance.\n",
        "\n",
        "Despite these changes, the image will still be labeled \"cat.\"\n",
        "\n",
        "Similarly, in text-based tasks, we can replace words with synonyms or translate the text to another language and then back to the original language.\n",
        "\n",
        "For audio tasks, we can amplify or reduce different frequency bands.\n",
        "\n",
        "The goal is to train the model to become robust against these irrelevant data transformations."
      ],
      "metadata": {
        "id": "5QRLWXVxa5Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3kfDeQjfUMII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp8ZVdVKUBCa"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "assert X_train.shape == (50000, 32, 32, 3)\n",
        "assert X_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to TensorFlow Datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# Data augmentation function\n",
        "def augment(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "    image = tf.image.random_hue(image, max_delta=0.2)\n",
        "    return image, label\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image / 127.5) - 1\n",
        "    return image, label\n",
        "\n",
        "# Dataset preparation function with augmentation option\n",
        "def prepare_dataset(ds, batch_size=100, augment_data=False):\n",
        "    if augment_data:\n",
        "        ds = ds.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    ds = ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    ds = ds.shuffle(buffer_size=10000).batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Preprocess the data\n",
        "train_ds = prepare_dataset(train_ds, augment_data=True)\n",
        "test_ds = prepare_dataset(test_ds, augment_data=False)"
      ],
      "metadata": {
        "id": "Px0jrrEaV2Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
        "        tf.keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "        tf.keras.layers.Dropout(0.3),  # Dropout layer\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=0.01, nesterov=True),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor=\"val_accuracy\", factor=0.2, patience=5, min_lr=0.001\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "callbacks = [early_stopping, reduce_lr]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds, epochs=100, validation_data=test_ds, batch_size=100, callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "ndLANVsQUWMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AQGT41TdY7lQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}