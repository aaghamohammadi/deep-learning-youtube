{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhExuVWTWSacG6pM+6LiXe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In our previous session, we delved into the topic of supervised learning.\n","\n","To formalize this concept, consider that we have an input tensor, denoted as $\\mathbf{x}$, and an output tensor, denoted as $\\mathbf{y}$.\n","\n","There exists a mathematical function $h: \\mathbf{X} â†’ \\mathbf{Y}$ that maps the input to the output.\n","\n","Our model includes parameters, represented by $\\boldsymbol{\\phi}$, which are currently unknown.\n","\n","The selection of these parameters establishes the specific relationship between the input and output. This relationship can be expressed as follows:\n","$$\\mathbf{y} = h\\left[\\mathbf{x}, \\boldsymbol{\\phi}\\right]$$\n","\n","The process of learning or training a model refers to the determination of the parameters $\\boldsymbol{\\phi}$ using the training data, which consists of pairs of inputs and outputs like so:\n","$$(x_1, y_1)$$\n","$$(x_2, y_2)$$\n","$$\\vdots$$\n","$$(x_m, y_m)$$"],"metadata":{"id":"mmqU-KxRXi87"}},{"cell_type":"markdown","source":["To simplify our discussion, let's concentrate on a single variable or one-dimensional scenario for the time being."],"metadata":{"id":"zKDXhi5Ja_5k"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","def plot_regression(x, y, dpi=150, color='red'):\n","    plt.figure(figsize=(5, 5), dpi=dpi)\n","    plt.scatter(x, y, color=color)\n","    plt.xlim(0, 2)\n","    plt.ylim(0, 2)\n","    plt.xticks(tf.range(0, 2.2, 0.2))\n","    plt.yticks(tf.range(0, 2.2, 0.2))\n","    plt.xlabel('Input, x')\n","    plt.ylabel('Output, y')\n","    plt.title('1D Linear Regression')\n","    plt.show()\n","\n","x = tf.constant([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90])\n","y = tf.constant([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ])\n","\n","plot_regression(x, y)"],"metadata":{"id":"DZqEf4SvaxHw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A one-dimensional linear regression model illustrates the correlation between the input x and the output y as a linear equation:\n","\n","$$y = \\phi_0 + \\phi_1 x$$"],"metadata":{"id":"-OBFsEopcgSN"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","def compute_output_1d(x, parameters):\n","    return parameters[0] + parameters[1] * x\n","\n","def plot_regression(x, y, parameters, colors, labels, dpi=150):\n","    x_values = tf.linspace(0, 2, 400)\n","    plt.figure(figsize=(5, 5), dpi=dpi)\n","\n","    for parameter, color, label in zip(parameters, colors, labels):\n","        y_values = compute_output_1d(x_values, parameter)\n","        plt.plot(x_values, y_values, label=label, color=color)\n","\n","    plt.scatter(x, y, color='red')\n","    plt.xlim(0, 2)\n","    plt.ylim(0, 2)\n","    plt.xticks(tf.range(0, 2.2, 0.2))\n","    plt.yticks(tf.range(0, 2.2, 0.2))\n","    plt.xlabel('Input, x')\n","    plt.ylabel('Output, y')\n","    plt.legend()\n","    plt.show()\n","\n","x = tf.constant([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90])\n","y = tf.constant([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ])\n","\n","parameters = [tf.constant([1.2, -0.1], dtype=tf.float64), tf.constant([0.0, 1.0], dtype=tf.float64), tf.constant([1.0, -0.4], dtype=tf.float64)]\n","colors = ['black', 'orange', 'cyan']\n","labels = [r'$\\phi_0=1.2, \\phi_1=-0.1$', r'$\\theta_0=0.0,\\theta_1=1.0$', r'$\\phi_0=1.0,\\phi_1=-0.4$']\n","\n","plot_regression(x, y, parameters, colors, labels)"],"metadata":{"id":"5-G8VPm1fWJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We require a systematic method to determine which parameters $\\boldsymbol{\\phi}$ are superior to others.\n","\n","For this purpose, we allocate a numerical score to each set of parameters that measures the discrepancy between the model and the data. This score is referred to as the loss; a smaller loss indicates a better match."],"metadata":{"id":"zZG6kTzIfbkn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EobbP1aPYNcn"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","def compute_output_1d(x, parameters):\n","    return parameters[0] + parameters[1] * x\n","\n","def draw_lines_to_model(x, y, parameters, color='gray', linestyle='--'):\n","    for xi, yi in zip(x, y):\n","        y_on_line = compute_output_1d(xi, parameters)\n","        plt.plot([xi, xi], [yi, y_on_line], color=color, linestyle=linestyle)\n","\n","def plot_regression(x, y, parameters, color, label, dpi=150):\n","    x_values = tf.linspace(0, 2, 400)\n","    plt.figure(figsize=(5, 5), dpi=dpi)\n","\n","    y_values = compute_output_1d(x_values, parameters)\n","    plt.plot(x_values, y_values, label=label, color=color)\n","\n","    plt.scatter(x, y, color='red')\n","    draw_lines_to_model(x, y, parameters)\n","    plt.xlim(0, 2)\n","    plt.ylim(0, 2)\n","    plt.xticks(tf.range(0, 2.2, 0.2))\n","    plt.yticks(tf.range(0, 2.2, 0.2))\n","    plt.xlabel('Input, x')\n","    plt.ylabel('Output, y')\n","    plt.legend()\n","    plt.show()\n","\n","x = tf.constant([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90], dtype=tf.float64)\n","y = tf.constant([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ], dtype=tf.float64)\n","\n","parameters = tf.constant([0.0, 1.0], dtype=tf.float64)\n","color = 'orange'\n","label = r'$\\theta_0=0.0,\\theta_1=1.0$'\n","\n","plot_regression(x, y, parameters, color, label)"]},{"cell_type":"markdown","source":["We can treat the loss as a function $L\\left[\\boldsymbol{\\phi}\\right]$ of these parameters. When we train the model, we are seeking parameters $\\boldsymbol{\\phi}$ that minimize this loss function:\n","\n","$$\\hat{\\phi} = \\underset{\\boldsymbol{\\phi}}{\\mathrm{argmin}} \\left[ L\\left[\\boldsymbol{\\phi}\\right] \\right]\n","$$"],"metadata":{"id":"5XBdW7KbhXGI"}},{"cell_type":"markdown","source":["For instance, in our single-variable scenario, we can formulate the loss function accordingly. This is referred to as the least-squares loss.\n","\n","$$ \\begin{align*}\n","L\\left[\\boldsymbol{\\phi}\\right] &= \\sum_{i=1}^{m} (h\\left[x_i, \\boldsymbol{\\phi}\\right] - y_i)^2 \\\\\n","&= \\sum_{i=1}^{m} (\\phi_0 + \\phi_1 x_i - y_i)^2\n","\\end{align*}\n","$$\n"],"metadata":{"id":"VJ-5oWhzjZli"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","x = tf.constant([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90], dtype=tf.float64)\n","y = tf.constant([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ], dtype=tf.float64)\n","\n","def compute_loss(x, y, phi0, phi1):\n","    # Compute the predicted values\n","    y_pred = phi0 + phi1 * x\n","    # Compute the squared differences\n","    squared_diffs = tf.square(y_pred - y)\n","    # Return the sum of the squared differences\n","    return tf.reduce_sum(squared_diffs)\n","\n","compute_loss(x, y, 0.0, 1)"],"metadata":{"id":"gp4HlqpZfR1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","def compute_loss_grid(x, y, phi0_values, phi1_values, compute_loss):\n","    phi0_grid, phi1_grid = np.meshgrid(phi0_values, phi1_values)\n","    loss_grid = np.zeros_like(phi0_grid)\n","    for i in range(phi0_grid.shape[0]):\n","        for j in range(phi0_grid.shape[1]):\n","            loss_grid[i, j] = compute_loss(x, y, phi0_grid[i, j], phi1_grid[i, j])\n","    return phi0_grid, phi1_grid, loss_grid\n","\n","def plot_loss_surface(phi0_grid, phi1_grid, loss_grid):\n","    fig = plt.figure(figsize=(10, 10), dpi=150)\n","    ax = fig.add_subplot(111, projection='3d')\n","    surf = ax.plot_surface(phi0_grid, phi1_grid, loss_grid, cmap='BrBG')\n","\n","    ax.set_xlabel('Intercept, $\\\\phi_0$')\n","    ax.set_ylabel('Slope, $\\\\phi_1$')\n","    ax.set_zlabel('Loss $L(\\\\phi)$')\n","    fig.colorbar(surf, shrink=0.5, aspect=10)\n","    plt.show()\n","\n","# Define the range of phi0 and phi1\n","phi0_values = np.linspace(0, 2, 100)\n","phi1_values = np.linspace(-1, 1, 100)\n","\n","# Compute the loss for each combination of phi0 and phi1\n","phi0_grid, phi1_grid, loss_grid = compute_loss_grid(x, y, phi0_values, phi1_values, compute_loss)\n","\n","# Plot the loss surface\n","plot_loss_surface(phi0_grid, phi1_grid, loss_grid)\n"],"metadata":{"id":"DNJLyg_Q3bRQ"},"execution_count":null,"outputs":[]}]}